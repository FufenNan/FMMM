{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self, extra_args=None):\n",
    "        print('mock:: opt')\n",
    "        if extra_args is not None:\n",
    "            for i in extra_args:\n",
    "                self.__dict__[i] = extra_args[i]\n",
    "args = Temp()\n",
    "\n",
    "args.dataname = args.dataset_name = 't2m'\n",
    "args.nb_code = 256\n",
    "args.code_dim = 32\n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.quantizer = 'ema_reset'\n",
    "args.mu = 0.99\n",
    "args.clip_dim = 512\n",
    "\n",
    "\n",
    "args.embed_dim_gpt = 1024\n",
    "args.block_size = 51\n",
    "args.num_layers = 9\n",
    "args.n_head_gpt = 16\n",
    "args.drop_out_rate = 0.1\n",
    "args.ff_rate = 4\n",
    "\n",
    "#转化为args\n",
    "args.vq_name='2024-08-19-00-17-48_vq_general_decoder'\n",
    "args.out_dir=\"output/t2m\"\n",
    "args.exp_name='trans_times'\n",
    "args.num_local_layer=2\n",
    "args.lr=0.0001\n",
    "args.nb_code=256\n",
    "args.code_dim=32\n",
    "args.embed_dim_gpt=512\n",
    "args.batch_size=512\n",
    "args.teacher_pth=\"/home/haoyum3/MMM/output/vq/2024-08-16-23-15-13_multi_vq_256_32_5/net_last.pth\"\n",
    "args.resume_pth=\"/home/haoyum3/MMM/output/vq/2024-08-19-00-17-48_vq_general_decoder/net_last.pth\"\n",
    "args.seed=1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22 21:49:20,646 INFO {\n",
      "    \"batch_size\": 512,\n",
      "    \"block_size\": 51,\n",
      "    \"clip_dim\": 512,\n",
      "    \"code_dim\": 32,\n",
      "    \"dataname\": \"t2m\",\n",
      "    \"dataset_name\": \"t2m\",\n",
      "    \"depth\": 3,\n",
      "    \"dilation_growth_rate\": 3,\n",
      "    \"down_t\": 2,\n",
      "    \"drop_out_rate\": 0.1,\n",
      "    \"embed_dim_gpt\": 512,\n",
      "    \"exp_name\": \"trans_times\",\n",
      "    \"ff_rate\": 4,\n",
      "    \"lr\": 0.0001,\n",
      "    \"mu\": 0.99,\n",
      "    \"n_head_gpt\": 16,\n",
      "    \"nb_code\": 256,\n",
      "    \"num_layers\": 9,\n",
      "    \"num_local_layer\": 2,\n",
      "    \"out_dir\": \"./output/t2m/2024-08-22-21-49-20_trans_times/\",\n",
      "    \"output_emb_width\": 512,\n",
      "    \"quantizer\": \"ema_reset\",\n",
      "    \"resume_pth\": \"/home/haoyum3/MMM/output/vq/2024-08-19-00-17-48_vq_general_decoder/net_last.pth\",\n",
      "    \"seed\": 1234,\n",
      "    \"stride_t\": 2,\n",
      "    \"teacher_pth\": \"/home/haoyum3/MMM/output/vq/2024-08-16-23-15-13_multi_vq_256_32_5/net_last.pth\",\n",
      "    \"vq_dir\": \"./output/vq/2024-08-19-00-17-48_vq_general_decoder\",\n",
      "    \"vq_name\": \"2024-08-19-00-17-48_vq_general_decoder\",\n",
      "    \"width\": 512\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:07<00:00, 199.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n",
      "Reading checkpoints/t2m/Comp_v6_KLD005/opt.txt\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "import options.option_transformer as option_trans\n",
    "from models.vqvae_multi import VQVAE_MULTI_V2\n",
    "from models.vqvae_general import HumanVQVAE_GENERAL,VQVAE_decode_only\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_timesformer as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.utils import get_model, visualize_2motions\n",
    "from tqdm import tqdm\n",
    "from exit.utils import get_model, visualize_2motions, generate_src_mask, init_save_folder, uniform, cosine_schedule\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "from exit.utils import base_dir\n",
    "\n",
    "##### ---- Exp dirs ---- #####\n",
    "# args = option_trans.get_args_parser()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# args.out_dir = os.path.join(args.out_dir, f'{args.exp_name}')\n",
    "init_save_folder(args)\n",
    "\n",
    "# [TODO] make the 'output/' folder as arg\n",
    "args.vq_dir = f'./output/vq/{args.vq_name}' #os.path.join(\"./dataset/KIT-ML\" if args.dataname == 'kit' else \"./dataset/HumanML3D\", f'{args.vq_name}')\n",
    "codebook_dir = f'{args.vq_dir}/codebook/'\n",
    "# args.resume_pth = f'{args.vq_dir}/net_last.pth'\n",
    "os.makedirs(args.vq_dir, exist_ok = True)\n",
    "os.makedirs(codebook_dir, exist_ok = True)\n",
    "os.makedirs(args.out_dir, exist_ok = True)\n",
    "os.makedirs(args.out_dir+'/html', exist_ok=True)\n",
    "\n",
    "##### ---- Logger ---- #####\n",
    "logger = utils_model.get_logger(args.out_dir)\n",
    "writer = SummaryWriter(args.out_dir)\n",
    "logger.info(json.dumps(vars(args), indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)\n",
    "\n",
    "dataset_opt_path = 'checkpoints/kit/Comp_v6_KLD005/opt.txt' if args.dataname == 'kit' else 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)\n",
    "\n",
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# https://github.com/openai/CLIP/issues/111\n",
    "class TextCLIP(torch.nn.Module):\n",
    "    def __init__(self, model) :\n",
    "        super(TextCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,text):\n",
    "        with torch.no_grad():\n",
    "            word_emb = self.model.token_embedding(text).type(self.model.dtype)\n",
    "            word_emb = word_emb + self.model.positional_embedding.type(self.model.dtype)\n",
    "            word_emb = word_emb.permute(1, 0, 2)  # NLD -> LND\n",
    "            word_emb = self.model.transformer(word_emb)\n",
    "            word_emb = self.model.ln_final(word_emb).permute(1, 0, 2).float()\n",
    "            enctxt = self.model.encode_text(text).float()\n",
    "        return enctxt, word_emb\n",
    "clip_model = TextCLIP(clip_model)\n",
    "# if args.teacher_pth:\n",
    "teacher_net= VQVAE_MULTI_V2(args, ## use args to define different parameters in different quantizers\n",
    "                        args.nb_code,#8192\n",
    "                        args.code_dim,#32\n",
    "                        args.output_emb_width,#512\n",
    "                        args.down_t,#2\n",
    "                        args.stride_t,#2\n",
    "                        args.width,#512\n",
    "                        args.depth,#3\n",
    "                        args.dilation_growth_rate,#3\n",
    "                        args.vq_act,#'relu'\n",
    "                        None,#None\n",
    "                        {'mean': torch.from_numpy(val_loader.dataset.mean).cuda().float(), \n",
    "                        'std': torch.from_numpy(val_loader.dataset.std).cuda().float()},\n",
    "                        True)\n",
    "    # logger.info('loading checkpoint from {}'.format(args.teacher_pth))\n",
    "    # teacher_ckpt=torch.load(args.teacher_pth, map_location='cpu')\n",
    "    # teacher_net.load_state_dict(teacher_ckpt['net'], strict=True)\n",
    "    # teacher_net.cuda()\n",
    "    # teacher_net.eval()\n",
    "net= VQVAE_decode_only(args, ## use args to define different parameters in different quantizers\n",
    "                        teacher_net,\n",
    "                        args.nb_code,#8192\n",
    "                        args.code_dim,#32\n",
    "                        args.output_emb_width,#512\n",
    "                        args.down_t,#2\n",
    "                        args.stride_t,#2\n",
    "                        args.width,#512\n",
    "                        args.depth,#3\n",
    "                        args.dilation_growth_rate,#3\n",
    "                        args.vq_act,#'relu'\n",
    "                        None,#None\n",
    "                        )\n",
    "print ('loading checkpoint from {}'.format(args.resume_pth))\n",
    "logger.info('loading checkpoint from {}'.format(args.resume_pth))\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "\n",
    "class VQVAE_WRAPPER(torch.nn.Module):\n",
    "    def __init__(self, vqvae) :\n",
    "        super().__init__()\n",
    "        self.vqvae = vqvae\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.vqvae(*args, **kwargs)\n",
    "net=VQVAE_WRAPPER(net)\n",
    "\n",
    "trans_encoder = trans.Text2Motion_Transformer(vqvae=net,\n",
    "                                num_vq=args.nb_code, \n",
    "                                embed_dim=args.embed_dim_gpt, \n",
    "                                clip_dim=args.clip_dim, \n",
    "                                block_size=args.block_size, \n",
    "                                num_layers=args.num_layers, \n",
    "                                num_local_layer=args.num_local_layer, \n",
    "                                n_head=args.n_head_gpt, \n",
    "                                drop_out_rate=args.drop_out_rate, \n",
    "                                fc_rate=args.ff_rate)\n",
    "\n",
    "if args.resume_trans is not None:\n",
    "    print ('loading transformer checkpoint from {}'.format(args.resume_trans))\n",
    "    ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "    trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
    "trans_encoder.train()\n",
    "trans_encoder.cuda()\n",
    "trans_encoder = torch.nn.DataParallel(trans_encoder)\n",
    "\n",
    "##### ---- Optimizer & Scheduler ---- #####\n",
    "optimizer = utils_model.initial_optim(args.decay_option, args.lr, args.weight_decay, trans_encoder, args.optimizer)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_scheduler, gamma=args.gamma)\n",
    "\n",
    "##### ---- Optimization goals ---- #####\n",
    "loss_ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "##### ---- get code ---- #####\n",
    "##### ---- Dataloader ---- #####\n",
    "if len(os.listdir(codebook_dir)) == 0:\n",
    "    train_loader_token = dataset_tokenize.DATALoader(args.dataname, 1, unit_length=2**args.down_t)\n",
    "    for batch in tqdm(train_loader_token,position=0, leave=True):\n",
    "        pose, name = batch\n",
    "        bs, seq = pose.shape[0], pose.shape[1]\n",
    "\n",
    "        pose = pose.cuda().float() # bs, nb_joints, joints_dim, seq_len\n",
    "        target = net.vqvae.teacher_net(pose, type='encode')\n",
    "        target = target.cpu().numpy()\n",
    "        np.save(pjoin(codebook_dir, name[0] +'.npy'), target)\n",
    "\n",
    "\n",
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, codebook_dir, unit_length=2**args.down_t,multi_sep=True)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)\n",
    "\n",
    "        \n",
    "##### ---- Training ---- #####\n",
    "best_fid=1000 \n",
    "best_iter=0 \n",
    "best_div=100 \n",
    "best_top1=0 \n",
    "best_top2=0 \n",
    "best_top3=0 \n",
    "best_matching=100 \n",
    "# pred_pose_eval, pose, m_length, clip_text, best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, best_multi, writer, logger = eval_trans.evaluation_transformer(args.out_dir, val_loader, net, trans_encoder, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, clip_model=clip_model, eval_wrapper=eval_wrapper)\n",
    "\n",
    "def get_acc(cls_pred, target, mask):\n",
    "    cls_pred = torch.masked_select(cls_pred, mask.unsqueeze(-1)).view(-1, cls_pred.shape[-1])\n",
    "    target_all = torch.masked_select(target, mask)\n",
    "    probs = torch.softmax(cls_pred, dim=-1)\n",
    "    _, cls_pred_index = torch.max(probs, dim=-1)\n",
    "    right_num = (cls_pred_index == target_all).sum()\n",
    "    return right_num*100/mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while nb_iter <= args.total_iter:\n",
    "for nb_iter in tqdm(range(1, args.total_iter + 1), position=0, leave=True):\n",
    "    batch = next(train_loader_iter)\n",
    "    #TODO receive multiple texts\n",
    "    clip_text, m_tokens, m_tokens_len = batch\n",
    "    m_tokens, m_tokens_len = m_tokens.cuda(), m_tokens_len.cuda()\n",
    "    bs = m_tokens.shape[0]\n",
    "    target = m_tokens    # (bs, 26)\n",
    "    target = target.cuda()\n",
    "    batch_size, max_len = target.shape[:2]\n",
    "\n",
    "    # Random Drop Text\n",
    "    # text_mask = np.random.random(len(clip_text)) > .05\n",
    "    # clip_text = np.array(clip_text)\n",
    "    # clip_text[~text_mask] = ''\n",
    "    \n",
    "    #[b,77]\n",
    "    text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "    #[bs,512],[bs,77,512]\n",
    "    feat_clip_text, word_emb = clip_model(text)\n",
    "\n",
    "    feat_clip_text = feat_clip_text.unsqueeze(-1).repeat(1, 1, 5)\n",
    "    # [INFO] Swap input tokens\n",
    "    if args.pkeep == -1:\n",
    "        proba = np.random.rand(1)[0]\n",
    "        mask = torch.bernoulli(proba * torch.ones(target.shape,\n",
    "                                                device=target.device))\n",
    "    else:\n",
    "        mask = torch.bernoulli(args.pkeep * torch.ones(target.shape,\n",
    "                                                device=target.device))\n",
    "    # random only motion token (not pad token). To prevent pad token got mixed up.\n",
    "    seq_mask_no_end = generate_src_mask(max_len, m_tokens_len).unsqueeze(-1).repeat(1,1,5)\n",
    "    mask = torch.logical_or(mask, ~seq_mask_no_end).int()\n",
    "    r_indices = torch.randint_like(target, args.nb_code)\n",
    "    input_indices = mask*target+(1-mask)*r_indices\n",
    "\n",
    "    # Time step masking\n",
    "    mask_id = get_model(net).vqvae.num_code + 2\n",
    "    # rand_time = uniform((batch_size,), device = target.device)\n",
    "    # rand_mask_probs = cosine_schedule(rand_time)\n",
    "    rand_mask_probs = torch.zeros(batch_size,device = m_tokens_len.device).float().uniform_(0.5, 1)\n",
    "    # rand_mask_probs = cosine_schedule(rand_mask_probs)\n",
    "    num_token_masked = (m_tokens_len * rand_mask_probs).round().clamp(min = 1)\n",
    "    batch_randperm = torch.rand((batch_size, max_len),device = target.device)\n",
    "    batch_randperm = batch_randperm.argsort(dim=-1)\n",
    "    mask_token = batch_randperm < rearrange(num_token_masked, 'b -> b 1')\n",
    "    mask_token = mask_token.unsqueeze(-1).repeat(1,1,5)\n",
    "\n",
    "    # masked_target = torch.where(mask_token, input=input_indices, other=-1)\n",
    "    masked_input_indices = torch.where(mask_token, mask_id, input_indices)\n",
    "\n",
    "    seq_mask = generate_src_mask(max_len, m_tokens_len+1).unsqueeze(-1).repeat(1,1,5)\n",
    "    att_txt = None # CFG: torch.rand((seq_mask.shape[0], 1)) > 0.1\n",
    "    cls_pred = trans_encoder(masked_input_indices, feat_clip_text, src_mask = seq_mask, att_txt=att_txt, word_emb=word_emb)[:, 1:]\n",
    "\n",
    "    # [INFO] Compute xent loss as a batch\n",
    "    weights = seq_mask_no_end / (seq_mask_no_end.sum(-1).unsqueeze(-1) * seq_mask_no_end.shape[0])\n",
    "    #TODO 改写attention head,输出confidence\n",
    "    cls_pred_seq_masked = cls_pred[seq_mask_no_end, :].view(-1, cls_pred.shape[-1])\n",
    "    target_seq_masked = target[seq_mask_no_end]\n",
    "    weight_seq_masked = weights[seq_mask_no_end]\n",
    "    loss_cls = F.cross_entropy(cls_pred_seq_masked, target_seq_masked, reduction = 'none')\n",
    "    loss_cls = (loss_cls * weight_seq_masked).sum()\n",
    "\n",
    "    ## global loss\n",
    "    optimizer.zero_grad()\n",
    "    loss_cls.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if nb_iter % args.print_iter ==  0 :\n",
    "        probs_seq_masked = torch.softmax(cls_pred_seq_masked, dim=-1)\n",
    "        _, cls_pred_seq_masked_index = torch.max(probs_seq_masked, dim=-1)\n",
    "        target_seq_masked = torch.masked_select(target, seq_mask_no_end)\n",
    "        right_seq_masked = (cls_pred_seq_masked_index == target_seq_masked).sum()\n",
    "\n",
    "        writer.add_scalar('./Loss/all', loss_cls, nb_iter)\n",
    "        writer.add_scalar('./ACC/every_token', right_seq_masked*100/seq_mask_no_end.sum(), nb_iter)\n",
    "        \n",
    "        # [INFO] log mask/nomask separately\n",
    "        no_mask_token = ~mask_token * seq_mask_no_end\n",
    "        writer.add_scalar('./ACC/masked', get_acc(cls_pred, target, mask_token), nb_iter)\n",
    "        writer.add_scalar('./ACC/no_masked', get_acc(cls_pred, target, no_mask_token), nb_iter)\n",
    "\n",
    "        # msg = f\"Train. Iter {nb_iter} : Loss. {avg_loss_cls:.5f}, ACC. {avg_acc:.4f}\"\n",
    "        # logger.info(msg)\n",
    "\n",
    "    if nb_iter==0 or nb_iter % args.eval_iter ==  0 or nb_iter == args.total_iter:\n",
    "        num_repeat = 1\n",
    "        rand_pos = False\n",
    "        if nb_iter == args.total_iter:\n",
    "            num_repeat = -30\n",
    "            rand_pos = True\n",
    "            val_loader = dataset_TM_eval.DATALoader(args.dataname, True, 32, w_vectorizer)\n",
    "        pred_pose_eval, pose, m_length, clip_text, best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, best_multi, writer, logger = eval_trans.evaluation_transformer(args.out_dir, val_loader, net, trans_encoder, logger, writer, nb_iter, best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, clip_model=clip_model, eval_wrapper=eval_wrapper, dataname=args.dataname, num_repeat=num_repeat, rand_pos=rand_pos)\n",
    "        # for i in range(4):\n",
    "        #     x = pose[i].detach().cpu().numpy()\n",
    "        #     y = pred_pose_eval[i].detach().cpu().numpy()\n",
    "        #     l = m_length[i]\n",
    "        #     caption = clip_text[i]\n",
    "        #     cleaned_name = '-'.join(caption[:200].split('/'))\n",
    "        #     visualize_2motions(x, val_loader.dataset.std, val_loader.dataset.mean, args.dataname, l, y, save_path=f'{args.out_dir}/html/{str(nb_iter)}_{cleaned_name}_{l}.html')\n",
    "\n",
    "    if nb_iter == args.total_iter: \n",
    "        msg_final = f\"Train. Iter {best_iter} : FID. {best_fid:.5f}, Diversity. {best_div:.4f}, TOP1. {best_top1:.4f}, TOP2. {best_top2:.4f}, TOP3. {best_top3:.4f}\"\n",
    "        logger.info(msg_final)\n",
    "        break   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
