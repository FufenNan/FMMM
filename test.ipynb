{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from os.path import join as pjoin\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import clip\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "import options.option_transformer as option_trans\n",
    "import models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "import utils.eval_trans as eval_trans\n",
    "from dataset import dataset_TM_train\n",
    "from dataset import dataset_TM_eval\n",
    "from dataset import dataset_tokenize\n",
    "import models.t2m_trans_multi as trans\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from exit.utils import get_model, visualize_2motions\n",
    "from tqdm import tqdm\n",
    "from exit.utils import get_model, visualize_2motions, generate_src_mask, init_save_folder, uniform, cosine_schedule\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "from exit.utils import base_dir\n",
    "from models.vqvae_multi_v2 import VQVAE_MULTI_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock:: opt\n"
     ]
    }
   ],
   "source": [
    "class Temp:\n",
    "    def __init__(self, extra_args=None):\n",
    "        print('mock:: opt')\n",
    "        if extra_args is not None:\n",
    "            for i in extra_args:\n",
    "                self.__dict__[i] = extra_args[i]\n",
    "args = Temp()\n",
    "\n",
    "args.dataname = args.dataset_name = 't2m'\n",
    "args.nb_code = 8192\n",
    "args.code_dim = 40\n",
    "args.output_emb_width = 512\n",
    "args.down_t = 2\n",
    "args.stride_t = 2\n",
    "args.width = 512\n",
    "args.depth = 3\n",
    "args.dilation_growth_rate = 3\n",
    "args.quantizer = 'ema_reset'\n",
    "args.mu = 0.99\n",
    "args.clip_dim = 512\n",
    "\n",
    "\n",
    "args.embed_dim_gpt = 1280\n",
    "args.block_size = 51\n",
    "args.num_layers = 9\n",
    "args.n_head_gpt = 16\n",
    "args.drop_out_rate = 0.1\n",
    "args.ff_rate = 4\n",
    "\n",
    "args.total_iter = 10\n",
    "args.batch_size = 1\n",
    "args.vq_name ='2024-08-12-23-14-09_vq5_multi'\n",
    "args.resume_trans='/home/haoyum3/MMM/output/t2m/2024-08-14-12-01-17_trans_multi_1/net_last.pth'\n",
    "args.resume_pth='/home/haoyum3/MMM/output/vq/2024-08-12-23-14-09_vq5_multi/net_last.pth'\n",
    "args.num_local_layer =2\n",
    "args.pkeep = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1460 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:02<00:00, 628.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer Pointing at 0\n"
     ]
    }
   ],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')\n",
    "val_loader = dataset_TM_eval.DATALoader(args.dataname, False, 32, w_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] make the 'output/' folder as arg\n",
    "args.vq_dir = f'/home/haoyum3/MMM/output/vq/{args.vq_name}' #os.path.join(\"./dataset/KIT-ML\" if args.dataname == 'kit' else \"./dataset/HumanML3D\", f'{args.vq_name}')\n",
    "codebook_dir = f'{args.vq_dir}/codebook/'\n",
    "args.resume_pth = f'{args.vq_dir}/net_last.pth'\n",
    "os.makedirs(args.vq_dir, exist_ok = True)\n",
    "os.makedirs(codebook_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [00:54<00:00, 428.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = dataset_TM_train.DATALoader(args.dataname, args.batch_size, args.nb_code, codebook_dir, unit_length=2**args.down_t, multi_sep=True)\n",
    "train_loader_iter = dataset_TM_train.cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint from /home/haoyum3/MMM/output/vq/2024-08-12-23-14-09_vq5_multi/net_last.pth\n",
      "loading transformer checkpoint from /home/haoyum3/MMM/output/t2m/2024-08-14-12-01-17_trans_multi_1/net_last.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text2Motion_Transformer(\n",
       "  (trans_base): CrossCondTransBase(\n",
       "    (vqvae): VQVAE_WRAPPER(\n",
       "      (vqvae): VQVAE_MULTI_V2(\n",
       "        (decoder_left_arm): Decoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(8, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): Conv1d(512, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (decoder_right_arm): Decoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(8, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): Conv1d(512, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (decoder_right_leg): Decoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(8, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): Conv1d(512, 59, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (decoder_left_leg): Decoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(8, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): Conv1d(512, 59, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (decoder_spine): Decoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(8, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): Conv1d(512, 60, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (decoder): Decoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(40, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (4): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): Conv1d(512, 263, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (encoder_left_arm): Encoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(48, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): Conv1d(512, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (encoder_right_arm): Encoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(48, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): Conv1d(512, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (encoder_right_leg): Encoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(59, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): Conv1d(512, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (encoder_left_leg): Encoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(59, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): Conv1d(512, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (encoder_spine): Encoder(\n",
       "          (model): Sequential(\n",
       "            (0): Conv1d(60, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (1): Resnet1D(\n",
       "                (model): Sequential(\n",
       "                  (0): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(9,), dilation=(9,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (1): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                  (2): ResConv1DBlock(\n",
       "                    (norm1): Identity()\n",
       "                    (norm2): Identity()\n",
       "                    (activation1): ReLU()\n",
       "                    (activation2): ReLU()\n",
       "                    (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                    (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): Conv1d(512, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "        )\n",
       "        (quantizer_left_arm): QuantizeEMAReset()\n",
       "        (quantizer_right_arm): QuantizeEMAReset()\n",
       "        (quantizer_right_leg): QuantizeEMAReset()\n",
       "        (quantizer_left_leg): QuantizeEMAReset()\n",
       "        (quantizer_spine): QuantizeEMAReset()\n",
       "      )\n",
       "    )\n",
       "    (learn_tok_emb): Embedding(3, 8)\n",
       "    (to_emb): Linear(in_features=40, out_features=1280, bias=True)\n",
       "    (cond_emb): Linear(in_features=512, out_features=1280, bias=True)\n",
       "    (pos_embedding): Embedding(51, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_embed): PositionEmbedding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (word_emb): Linear(in_features=512, out_features=1280, bias=True)\n",
       "    (cross_att): Sequential(\n",
       "      (0): Block_crossatt(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CrossAttention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block_crossatt(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CrossAttention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans_head): CrossCondTransHead(\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=256, out_features=8192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VQVAE_MULTI_V2(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,#40\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate,\n",
    "                       moment={'mean': torch.from_numpy(val_loader.dataset.mean).cuda().float(), \n",
    "                        'std': torch.from_numpy(val_loader.dataset.std).cuda().float()},\n",
    "                       sep_decoder=True)\n",
    "                       \n",
    "class VQVAE_WRAPPER(torch.nn.Module):\n",
    "    def __init__(self, vqvae) :\n",
    "        super().__init__()\n",
    "        self.vqvae = vqvae\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.vqvae(*args, **kwargs)\n",
    "print ('loading checkpoint from {}'.format(args.resume_pth))\n",
    "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "net = VQVAE_WRAPPER(net)\n",
    "\n",
    "\n",
    "trans_encoder = trans.Text2Motion_Transformer(vqvae=net,\n",
    "                                num_vq=args.nb_code, \n",
    "                                embed_dim=args.embed_dim_gpt, \n",
    "                                clip_dim=args.clip_dim, \n",
    "                                block_size=args.block_size, \n",
    "                                num_layers=args.num_layers, \n",
    "                                num_local_layer=args.num_local_layer, \n",
    "                                n_head=args.n_head_gpt, \n",
    "                                drop_out_rate=args.drop_out_rate, \n",
    "                                fc_rate=args.ff_rate)\n",
    "\n",
    "# [TODO] DP doesn't work with single sample of vqvae decoder in eval\n",
    "# [TODO] move to [1stage] VQ stage\n",
    "# net = torch.nn.DataParallel(net)\n",
    "net.eval()\n",
    "net.cuda()\n",
    "\n",
    "if args.resume_trans is not None:\n",
    "    print ('loading transformer checkpoint from {}'.format(args.resume_trans))\n",
    "    ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
    "    trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
    "trans_encoder.eval()\n",
    "trans_encoder.cuda()\n",
    "#trans_encoder = torch.nn.DataParallel(trans_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_target(idx_target,idx_pred,seq_mask_no_end,batch_size,max_len,m_tokens_len,mask_id):\n",
    "    if args.pkeep == -1:\n",
    "        proba = np.random.rand(1)[0]\n",
    "        mask = torch.bernoulli(proba * torch.ones(idx_target[...,idx_pred].shape, device=idx_target.device))\n",
    "    else:\n",
    "        mask = torch.bernoulli(args.pkeep * torch.ones(idx_target[...,idx_pred].shape, device=idx_target.device))\n",
    "    mask = torch.logical_or(mask, ~seq_mask_no_end).int()\n",
    "    r_indices = torch.randint_like(idx_target[...,idx_pred], args.nb_code)\n",
    "    input_indices = mask * idx_target[...,idx_pred] + (1 - mask) * r_indices\n",
    "\n",
    "    rand_mask_probs = torch.zeros(batch_size, device = m_tokens_len.device).float().uniform_(0.5, 1)\n",
    "    num_token_masked = (m_tokens_len * rand_mask_probs).round().clamp(min = 1)\n",
    "    batch_randperm = torch.rand((batch_size, max_len), device = idx_target.device) - seq_mask_no_end.int()\n",
    "    batch_randperm = batch_randperm.argsort(dim = -1)\n",
    "    mask_token = batch_randperm < rearrange(num_token_masked, 'b -> b 1')\n",
    "\n",
    "    masked_input_indices = torch.where(mask_token, mask_id, input_indices)\n",
    "    idx_target[...,idx_pred] = masked_input_indices\n",
    "    return mask_token\n",
    "\n",
    "def mask_else(idx_target,idx_pred,seq_mask_no_end,batch_size,max_len,mask_id):\n",
    "    proba = torch.randint(low=0, high=11, size=(idx_target.shape[0],))/10\n",
    "    proba = proba[:, None].cuda()\n",
    "    mask = torch.bernoulli(proba * torch.ones(idx_target[...,idx_pred].shape,device=idx_target.device))\n",
    "    mask = torch.logical_or(mask, ~seq_mask_no_end).int().unsqueeze(-1).repeat(1,1,5)\n",
    "    mask[idx_pred] = torch.zeros_like(mask[idx_pred])\n",
    "    input_indices = mask*idx_target+(1-mask)*mask_id\n",
    "    idx_target = input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ---- Network ---- #####\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# https://github.com/openai/CLIP/issues/111\n",
    "class TextCLIP(torch.nn.Module):\n",
    "    def __init__(self, model) :\n",
    "        super(TextCLIP, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,text):\n",
    "        with torch.no_grad():\n",
    "            word_emb = self.model.token_embedding(text).type(self.model.dtype)\n",
    "            word_emb = word_emb + self.model.positional_embedding.type(self.model.dtype)\n",
    "            word_emb = word_emb.permute(1, 0, 2)  # NLD -> LND\n",
    "            word_emb = self.model.transformer(word_emb)\n",
    "            word_emb = self.model.ln_final(word_emb).permute(1, 0, 2).float()\n",
    "            enctxt = self.model.encode_text(text).float()\n",
    "        return enctxt, word_emb\n",
    "clip_model = TextCLIP(clip_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_pred 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m seq_mask \u001b[38;5;241m=\u001b[39m generate_src_mask(max_len, m_tokens_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m mask_token\u001b[38;5;241m=\u001b[39mmask_target(target,idx_pred,seq_mask_no_end,batch_size,max_len,m_tokens_len,mask_id)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmask_else\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq_mask_no_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Random Drop Text\u001b[39;00m\n\u001b[1;32m     23\u001b[0m text_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom(\u001b[38;5;28mlen\u001b[39m(clip_text)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m.1\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mmask_else\u001b[0;34m(idx_target, idx_pred, seq_mask_no_end, batch_size, max_len, mask_id)\u001b[0m\n\u001b[1;32m     24\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(proba \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(idx_target[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,idx_pred]\u001b[38;5;241m.\u001b[39mshape,device\u001b[38;5;241m=\u001b[39midx_target\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m     25\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_or(mask, \u001b[38;5;241m~\u001b[39mseq_mask_no_end)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m mask[idx_pred] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_pred\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     27\u001b[0m input_indices \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m*\u001b[39midx_target\u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mmask)\u001b[38;5;241m*\u001b[39mmask_id\n\u001b[1;32m     28\u001b[0m idx_target \u001b[38;5;241m=\u001b[39m input_indices\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "gt_token =[]\n",
    "pred_token = []\n",
    "for nb_iter in tqdm(range(1, args.total_iter + 1), position=0, leave=True):\n",
    "    batch = next(train_loader_iter)\n",
    "    clip_text, m_tokens, m_tokens_len = batch\n",
    "    m_tokens, m_tokens_len = m_tokens.cuda(), m_tokens_len.cuda()\n",
    "    bs = m_tokens.shape[0]\n",
    "    target = m_tokens    # (bs, 26)\n",
    "    target = target.cuda()\n",
    "    idx_pred=torch.randint(0, 5, (1,)).item()\n",
    "    print('idx_pred',idx_pred)\n",
    "    target_pred= target[..., idx_pred].clone()\n",
    "    gt_token.append(target.clone())\n",
    "    pred_token.append(target.clone())\n",
    "    mask_id = get_model(net).vqvae.num_code + 2\n",
    "    batch_size, max_len = target.shape[:2]\n",
    "    seq_mask_no_end = generate_src_mask(max_len, m_tokens_len)\n",
    "    seq_mask = generate_src_mask(max_len, m_tokens_len+1)\n",
    "    mask_token=mask_target(target,idx_pred,seq_mask_no_end,batch_size,max_len,m_tokens_len,mask_id)\n",
    "    mask_else(target, idx_pred,seq_mask_no_end,batch_size,max_len,mask_id)\n",
    "\n",
    "    # Random Drop Text\n",
    "    text_mask = np.random.random(len(clip_text)) > .1\n",
    "    clip_text = np.array(clip_text)\n",
    "    clip_text[~text_mask] = ''\n",
    "    \n",
    "    text = clip.tokenize(clip_text, truncate=True).cuda()\n",
    "    feat_clip_text, word_emb = clip_model(text)\n",
    "    att_txt = None #proba != 1 # CFG: torch.rand((seq_mask.shape[0], 1)) > 0.1\n",
    "    #[bs,50,8092]\n",
    "    cls_pred = trans_encoder(target, feat_clip_text, src_mask = seq_mask, att_txt=att_txt, word_emb=word_emb)[:, 1:] #, txt_mark=txt_mark\n",
    "    cls_pred = cls_pred[:,:,idx_pred,:].squeeze(2)\n",
    "\n",
    "    # [INFO] Compute xent loss as a batch\n",
    "    weights = seq_mask_no_end / (seq_mask_no_end.sum(-1).unsqueeze(-1) * seq_mask_no_end.shape[0])\n",
    "    cls_pred_seq_masked = cls_pred[seq_mask_no_end, :].view(-1, cls_pred.shape[-1])\n",
    "    target_seq_masked = target_pred[seq_mask_no_end]\n",
    "    weight_seq_masked = weights[seq_mask_no_end]\n",
    "    num_classes = cls_pred.shape[-1]\n",
    "\n",
    "    probs_seq_masked = torch.softmax(cls_pred_seq_masked, dim=-1)\n",
    "    _, cls_pred_seq_masked_index = torch.max(probs_seq_masked, dim=-1)\n",
    "    target_seq_masked = torch.masked_select(target_pred, seq_mask_no_end)\n",
    "    right_seq_masked = (cls_pred_seq_masked_index == target_seq_masked).sum()\n",
    "    print(cls_pred_seq_masked_index.shape)\n",
    "    print(pred_token[-1].shape)\n",
    "    pred_token[-1][seq_mask_no_end, idx_pred] = cls_pred_seq_masked_index\n",
    "    print('ACC/every_token', right_seq_masked*100/seq_mask_no_end.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt,pred in zip(gt_token,pred_token):\n",
    "    pred_pose = net(pred_token, type='decode')\n",
    "    gt_pose = net(gt_token, type='decode')\n",
    "    _motions = pred_pose_upper[i, :m_length[i]].detach().cpu().numpy() * val_loader.dataset.std + val_loader.dataset.mean\n",
    "    _motions = recover_from_ric(torch.from_numpy(_motions).float(), 22).numpy()\n",
    "    animate3d(_motions, BONE_LINK=t2m_bone, first_total_standard=27, axis_visible=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
